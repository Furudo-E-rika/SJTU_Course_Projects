{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# n元语言模型回退算法\n",
    "\n",
    "本次作业要求补全本笔记中的n元语言模型的采用Good-Turing折扣的Katz回退算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P_{\\text{bo}}(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) P_{\\text{bo}}(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现n元语言模型及采用Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n",
    "\n",
    "$d$与$\\alpha$如何计算可以参考作业文件中的算法说明以及[SRILM](http://www.speech.sri.com/projects/srilm/)的[`ngram-discount(7)`手册页](http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea6708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.n: int = n\n",
    "\n",
    "        self.frequencies: List[Dict[Gram, int]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "        self.disfrequencies: List[Dict[Gram, int]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.ncounts: Dict[ Gram\n",
    "                          , Dict[int, int]\n",
    "                          ] = {}\n",
    "        self.Nr : Dict[int, int] = {}\n",
    "        self.discount_threshold:int = 7\n",
    "        self._d: Dict[Gram, Tuple[float, float]] = {}\n",
    "        self._alpha: List[Dict[Gram, float]]\\\n",
    "            = [{} for _ in range(n)]\n",
    "\n",
    "        self.eps = 1e-10\n",
    "        self.sum = 0\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "\n",
    "        for stc in corpus:\n",
    "            for i in range(1, len(stc)+1):\n",
    "                for j in range(min(i, self.n)):\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    gram = tuple(stc[i-j-1:i])\n",
    "                    if gram in self.frequencies[j]:\n",
    "                        self.frequencies[j][gram] += 1\n",
    "                    else:\n",
    "                        self.frequencies[j][gram] = 1\n",
    "                    \n",
    "                    \n",
    "\n",
    "        for i in range(1, self.n):\n",
    "            grams = itertools.groupby(\n",
    "                    sorted(\n",
    "                        sorted(\n",
    "                            map(lambda itm: (itm[0][:-1], itm[1]),\n",
    "                                 self.frequencies[i].items()),\n",
    "                               key=(lambda itm: itm[1])),    \n",
    "                        key=(lambda itm: itm[0]))) \n",
    "                        #  该grouby函数有两个返回值，\n",
    "                        #  其中第一个返回值的第一项表示了W_k之前的n-1个前序词信息, 第二个返回值表示该序列的频率\n",
    "                        #  第一个返回值的迭代个数表示了以该前序出现的词的个数\n",
    "\n",
    "            # TODO: calculates the value of $N_r$\n",
    "            for past, num in grams :\n",
    "                \n",
    "                length = 0\n",
    "                past_tuple = past[0]\n",
    "                fre = past[1]\n",
    "                for i in num:\n",
    "                    length += 1\n",
    "\n",
    "                if past_tuple in self.ncounts :\n",
    "                    self.ncounts[past_tuple][fre] = length\n",
    "                else :\n",
    "                    self.ncounts[past_tuple] = {}\n",
    "                    self.ncounts[past_tuple][fre] = length\n",
    "\n",
    "        self.sum = 0\n",
    "        for g in self.frequencies[0]:\n",
    "            self.sum += self.frequencies[0][g]\n",
    "        self.sum = float(self.sum)\n",
    "        ## 计算1-gram的出现频率总和，将用于之后的__getitem__函数\n",
    "\n",
    "        for i in range(self.n):\n",
    "            for (gram, fre) in self.frequencies[i].items():\n",
    "                if fre in self.Nr:\n",
    "                    self.Nr[fre] += 1\n",
    "                else:\n",
    "                    self.Nr[fre] = 1\n",
    "        ## 统计Nr\n",
    "\n",
    "        return self.frequencies, self.ncounts\n",
    "        ## 函数的第一个返回值记录了了不同n值（1至self.n）的n-gram以及其对应的出现频率\n",
    "        ## 第二个返回值的第一个索引值表示的是可能的前序词，第二个索引值表示的是这个前序词出现的频率（r），\n",
    "        ## 而最终得到的值即该前序词下该频率（r）的n-grams的出现次数(N_{r})\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        if gram not in self._d:\n",
    "            # TODO: calculates the value of $d'$\n",
    "            r = self.frequencies[len(gram)-1].get(gram, 0)\n",
    "            assert r > 0\n",
    "            if r > self.discount_threshold:   ## 若 C(W^{k-1}_{k-n+1})大于阈值，直接将d设为1\n",
    "                self._d[gram] = 1\n",
    "                return self._d[gram]\n",
    "            else:\n",
    "\n",
    "                lab = self.Nr[1] / (self.Nr[1] - (self.discount_threshold + 1) * self.Nr[self.discount_threshold + 1])\n",
    "                N_r = self.Nr[r]\n",
    "                N_r_1 = self.Nr[r+1]\n",
    "                self._d[gram] = (lab * (r+1) * N_r_1 ) / (r*N_r) + (1-lab)\n",
    "                # self._d[gram] = (numerator1/denominator, - numerator2/denominator)\n",
    "\n",
    "                # assert self._d[gram] >= 0 \n",
    "          \n",
    "        return self._d[gram]\n",
    "\n",
    "                ## 由于没有理解原函数给出的两个返回值的意义，我在自己理解的基础上将返回值改成了一个\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram) \n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n-1]: ## gram == W^{k-1}_{k-n+1}\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "\n",
    "                numerator = 0\n",
    "                denominator = 0\n",
    "\n",
    "                for gram_n in self.frequencies[n]:\n",
    "                    if gram == gram_n[:-1]:  ## gram_n即在W^{k-1}_{k-n+1}的基础上加上w_k\n",
    "                        ## V_plus\n",
    "                        numerator +=  self.__getitem__(gram_n)  ## P_{bo}(w_k | W^{k-1}_{k-n+1})\n",
    "                        denominator += self.__getitem__(gram_n[1:])  ## P_{bo}(w_k | W^{k-1}_{k-n+2})\n",
    "\n",
    "                numerator = 1 - numerator\n",
    "                denominator = 1 - denominator\n",
    "                \n",
    "                # assert numerator > 0 and denominator > 0\n",
    "                self._alpha[n][gram] = numerator/denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)-1\n",
    "\n",
    "\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n>0:\n",
    "                # TODO: calculates the smoothed probability value according to the formular\n",
    "                if self.frequencies[n].get(gram, 0) > self.eps: ## C(W^{k}_{k-n+1}) > 0\n",
    "                    self.disfrequencies[n][gram] = self.d(gram) * self.frequencies[n][gram] / self.frequencies[n-1][gram[:-1]]\n",
    "                else:\n",
    "                    self.disfrequencies[n][gram] = self.alpha(gram[:-1]) * self.__getitem__(gram[1:]) \n",
    "            else:\n",
    "                self.disfrequencies[n][gram] = self.frequencies[n].get(gram, self.eps) / self.sum\n",
    "                \n",
    "        # assert self.disfrequencies[n][gram] > 0\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        for i in range(2, len(sentence)+1):\n",
    "            # TODO: calculates the log probability\n",
    "            j = min(i, self.n)\n",
    "            gram = tuple(sentence[i-j:i]) \n",
    "            log_prob += math.log2(self.__getitem__(gram)) \n",
    "            ## 遍历所有长度为n的grams，累加其P值的对数\n",
    "        log_prob *= (-1 / len(sentence))\n",
    "        return log_prob\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        PPL = 1\n",
    "        for i in range(2,len(sentence)+1):\n",
    "            j = min(i, self.n)\n",
    "            gram = tuple(sentence[i-j:i])\n",
    "            PPL *= (1 / self.__getitem__(gram))\n",
    "            ## 遍历所有长度为n的grams，累乘其P值的倒数\n",
    "        print(PPL,1/float(len(sentence)-1))\n",
    "\n",
    "        PPL = math.pow(PPL, 1/float(len(sentence)-1))\n",
    "        # TODO: calculates the PPL\n",
    "        return PPL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.587287847661497e+34 0.125\n",
      "22508.073785375276\n",
      "5.999742452490332e+19 0.14285714285714285\n",
      "669.0329624144102\n",
      "1.0942025201104746e+27 0.09090909090909091\n",
      "287.1440303760335\n",
      "1.496178395269224e+64 0.030303030303030304\n",
      "88.0433311831805\n",
      "2.1492655051770874e+31 0.14285714285714285\n",
      "29925.501581854347\n",
      "9.482145093731398e+21 0.07692307692307693\n",
      "49.03783371751199\n",
      "1.6093235170018954e+35 0.03333333333333333\n",
      "14.912648403472929\n",
      "2.246515784658003e+57 0.038461538461538464\n",
      "160.6302323186319\n",
      "2.0749654887986508e+89 0.03333333333333333\n",
      "948.9289491259567\n",
      "8.760193318442967e+45 0.04\n",
      "68.81776241881947\n",
      "8.168739512745794e+21 0.1111111111111111\n",
      "272.07202820749205\n",
      "8.265477899154348e+43 0.047619047619047616\n",
      "123.39525923634163\n",
      "1.212542289319791e+59 0.0625\n",
      "4928.684923860772\n",
      "1.4347888773107825e+70 0.045454545454545456\n",
      "1545.0584343284559\n",
      "2.5662332513794994e+63 0.041666666666666664\n",
      "438.5852332782527\n",
      "4.3079844789677725e+35 0.07142857142857142\n",
      "350.9985278769381\n",
      "172489293.29471415 0.5\n",
      "13133.517932934577\n",
      "1.1678045076348033e+79 0.058823529411764705\n",
      "44773.57508604665\n",
      "8.802159283842638e+34 0.058823529411764705\n",
      "113.64859523632715\n",
      "25371409514069.906 0.16666666666666666\n",
      "171.41839818677815\n",
      "2.7690604636183504e+51 0.05555555555555555\n",
      "720.9536712871318\n",
      "4.032418348050249e+19 0.16666666666666666\n",
      "1851.800784282767\n",
      "1.3923092215776774e+100 0.02564102564102564\n",
      "369.6477736754964\n",
      "1.6321614609401934e+48 0.05263157894736842\n",
      "344.7575996977342\n",
      "7.605210091958161e+65 0.029411764705882353\n",
      "86.63227945581605\n",
      "2.5306402485696116e+42 0.045454545454545456\n",
      "84.60958485262762\n",
      "2.2707308572709546e+74 0.03571428571428571\n",
      "452.4570348518224\n",
      "990490.3778710853 0.16666666666666666\n",
      "9.984087461712802\n",
      "3557256898830063.0 0.14285714285714285\n",
      "166.56658816500118\n",
      "5.58225716969871e+60 0.04\n",
      "269.07440810220095\n",
      "2.0808990560641457e+31 0.07692307692307693\n",
      "256.50523426525496\n",
      "2.5758230241765094e+26 0.1\n",
      "437.61439775185005\n",
      "3.343116121233884e+63 0.03125\n",
      "96.63395024489536\n",
      "6.899531609118811e+48 0.045454545454545456\n",
      "165.93818893379736\n",
      "1.2776303568844794e+24 0.125\n",
      "1031.0996816698248\n",
      "6.3266132746121306e+35 0.05263157894736842\n",
      "76.60765579412337\n",
      "1.524180754330306e+92 0.03125\n",
      "759.8360416537789\n",
      "4.0045777624648155e+66 0.041666666666666664\n",
      "595.8082746835078\n",
      "1.065631883840052e+25 0.1\n",
      "318.24436358314796\n",
      "2.7281659375557473e+18 0.2\n",
      "4866.022989818294\n",
      "2.236045981906432e+24 0.09090909090909091\n",
      "163.52688246570054\n",
      "1.404414271897213e+57 0.05263157894736842\n",
      "1018.0354636172617\n",
      "4.593443487018118e+77 0.02702702702702703\n",
      "125.59626165350697\n",
      "1.637752307127203e+26 0.07692307692307693\n",
      "103.86772807693829\n",
      "2.5359592309612713e+110 0.029411764705882353\n",
      "1766.7725981375802\n",
      "2.9868400406339035e+41 0.07692307692307693\n",
      "1550.2470741705451\n",
      "5.9598225727730425e+62 0.041666666666666664\n",
      "412.70024064528775\n",
      "4.2009318714875986e+48 0.0625\n",
      "1093.8533530337345\n",
      "7.636257961097793e+20 0.1\n",
      "122.54287328331822\n",
      "2.067315186311648e+61 0.041666666666666664\n",
      "358.763797444289\n",
      "Avg:  2804.9555679827836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#with open(\"model.pkl\", \"rb\") as f:\n",
    "#    vocabulary = pkl.load(f)\n",
    "#    model = pkl.load(f)\n",
    "#print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\", encoding=\"utf-8\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2416e4fe6ad35e923dfeb1d776a83cc630c28a91844e4bc69c3e449d61140dfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
